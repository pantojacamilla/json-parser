    import findspark&#xD;&#xA;    findspark.init('/home/user1/spark')&#xD;&#xA;    from pyspark import SparkConf, SparkContext&#xD;&#xA;    from operator import add&#xD;&#xA;    import sys&#xD;&#xA;    from pyspark.streaming import StreamingContext&#xD;&#xA;    from pyspark.sql import Column, DataFrame, Row, SparkSession&#xD;&#xA;    from pyspark.streaming.kafka import KafkaUtils&#xD;&#xA;    import json&#xD;&#xA;    from kafka import SimpleProducer, KafkaClient&#xD;&#xA;    from kafka import KafkaProducer&#xD;&#xA;    producer = KafkaProducer(bootstrap_servers='server.kafka:9092')&#xD;&#xA;    def handler(message):&#xD;&#xA;    	records = message.collect()&#xD;&#xA;    	for record in records:&#xD;&#xA;    		producer.send('spark.out', str(record))&#xD;&#xA;    		print(record)&#xD;&#xA;    		producer.flush()&#xD;&#xA;    def main():&#xD;&#xA;    	sc = SparkContext(appName="PythonStreamingDirectKafkaWordCount")&#xD;&#xA;    	ssc = StreamingContext(sc, 1)&#xD;&#xA;    	lines = ssc.textFileStream('/home/user1/files/')&#xD;&#xA;    	fields = lines.map(lambda l: l.split(",")) &#xD;&#xA;    	udr =  fields.map(lambda p: Row(Name=p[0],Age=int(p[3].split('@')[0]),MailId=p[31],Address=p[29],Phone=p[46]))&#xD;&#xA;    	udr.foreachRDD(handler)&#xD;&#xA;    	ssc.start()&#xD;&#xA;    	ssc.awaitTermination()&#xD;&#xA;    if __name__ == "__main__":&#xD;&#xA;    	main()